{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.2 discriminative_model_0827_Lulu.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T5Z25eo1avbL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noo-rashbass/synthetic-data-service/blob/master/Evaluation/discriminative_model_NEW_Lulu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPtJv5RdteZq",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Discriminative Model\n",
        "For a quantitative measure of similarity, we train a post-hoc time-series classification model (by optimizing a multi-layer GRU) to distinguish between sequences from the original and generated datasets. First, each original sequence is labeled **'1'**, and each generated sequence is labeled **'0'**. Then, an off-the-shelf (RNN) classifier is trained to distinguish between the two classes as a standard supervised task. We then report the classification error on the held-out test set, which gives a quantitative assessment of fidelity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PieEO70OaR7C",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kil-9jsdtDHD"
      },
      "source": [
        "Understanding the <!--[text](link)-->[classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5Z25eo1avbL"
      },
      "source": [
        "# Functions for Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jLS9M8Q1zAzq"
      },
      "source": [
        "Lulu's Notes:\n",
        "\n",
        "* separated data loading into functions\n",
        "* returned `hidden_dim` \n",
        "* replaced `mix_divide()` with `train_val_test_split()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMb9XcNja-wC",
        "colab": {}
      },
      "source": [
        "# def reshape_removena_stack(ori_data):\n",
        "#   ori_data = np.split(ori_data, np.shape(ori_data)[0]/10, axis=0)\n",
        "#   ori_data_new = []\n",
        "#   for array in ori_data:\n",
        "#     if not np.isnan(array).any():\n",
        "#       ori_data_new.append(array)\n",
        "#   return ori_data_new\n",
        "\n",
        "# def load_DoppelGANger():\n",
        "#   ori_data = np.load('synthetic data/doppelGANger/ori_features_prism.npy') #Aisha: Change the path of loaded data for consistency\n",
        "#   gen_data = np.load('synthetic data/doppelGANger/features_600.npy')\n",
        "#   return ori_data, gen_data\n",
        "\n",
        "# def load_tGAN():\n",
        "#   ori_data = pd.read_csv('synthetic data/TGAN/cat_time_10visits_all_noid.csv').values # shape (12390 patients visits, 10 features)\n",
        "#   ori_data = reshape_removena_stack(ori_data) # shape (841 patients, 10 visits, 10 features)\n",
        "#   gen_data = np.load('synthetic data/TGAN/gen_cat_time_10visits_wl_5000it.npy')[:np.shape(ori_data)[0]] # shape (841 patients, 10 visits, 10 features)\n",
        "#   return ori_data, gen_data\n",
        "\n",
        "def load_DoppelGANger():\n",
        "  ori_data = pd.read_csv('/content/cat_time_5abovevisits_all.csv') # max timeseries length = 130\n",
        "  # gen_data = pd.read_csv('/content/gen_doptf2_cat_5abovevisits_e100_lstm.csv') # max timeseries length = 107\n",
        "  gen_data = pd.read_csv('/content/gen_doptf2_cat_5abovevisits_e200_lstm.csv') # max timeseries length = 111\n",
        "  ori_data = ori_data.drop(columns=['diar_No', 'diar_Yes', 'head_No', 'head_Yes'])\n",
        "  gen_data = gen_data.drop(columns=['diar_No', 'diar_Yes', 'head_No', 'head_Yes'])\n",
        "  ori_data = cat_df_to_3d_array(ori_data, 130) # array (1347, 130, 10)\n",
        "  gen_data = cat_df_to_3d_array(gen_data, 130) # array (1347, 130, 10)\n",
        "  return np.nan_to_num(ori_data), np.nan_to_num(gen_data)\n",
        "\n",
        "def cat_df_to_3d_array(data, max_length):\n",
        "  data.fillna(0)\n",
        "  # max_length = data['id'].value_counts().max() # if you want to get it from the data, but ori/gen may have different max lengths\n",
        "  lst = []\n",
        "  for i in data.id.unique():\n",
        "    timeseries = data[data['id']==i].drop(columns='id').to_numpy()\n",
        "    length = np.shape(timeseries)[0]\n",
        "    timeseries = np.pad(timeseries, pad_width=((0,max_length-length), (0,0)), mode='constant') # fill remaining rows with zeros\n",
        "    lst.append(timeseries)\n",
        "  array = np.stack(lst)\n",
        "  return array"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K75wQCZkdyoX",
        "colab": {}
      },
      "source": [
        "def MinMaxScaler(data): # This is a normalisation method copied from TGANs code # Lulu: not used\n",
        "  \"\"\"Min Max normalizer.\n",
        "  \n",
        "  Args:\n",
        "    - data: original data\n",
        "  \n",
        "  Returns:\n",
        "    - norm_data: normalized data\n",
        "  \"\"\"\n",
        "  numerator = data - np.min(data, 0)\n",
        "  denominator = np.max(data, 0) - np.min(data, 0)\n",
        "  norm_data = numerator / (denominator + 1e-7)\n",
        "  return norm_data\n",
        "\n",
        "\n",
        "def InputSize(ori_data): # Set the input size to the model\n",
        "    no, seq_len, dim = np.asarray(ori_data).shape \n",
        "    hidden_dim = int(dim/2)\n",
        "    input_dim = [None,dim]\n",
        "    return input_dim, hidden_dim # Lulu: added hidden_dim and renamed input_size because of later conflict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_whodMSUeLix",
        "colab": {}
      },
      "source": [
        "def train_val_test_split(ori_data, gen_data, rate=(0.65, 0.2, 0.15)): # Lulu: using sklearn, replaces mix_divide\n",
        "  # rate = (train, val, test) must sum to one\n",
        "\n",
        "  data = np.concatenate([ori_data,gen_data],axis=0)\n",
        "  labels = np.concatenate([np.ones(len(ori_data)), np.zeros(len(gen_data))], axis=0)\n",
        "\n",
        "  train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=rate[2])\n",
        "  train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, train_size=rate[0]/(rate[0]+rate[1]))\n",
        "  return train_data, val_data, test_data, train_labels, val_labels, test_labels"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yY46QrK-edLZ"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wEOYq9ZKxkHd"
      },
      "source": [
        "Lulu's Notes:\n",
        "\n",
        "* No normalisation used\n",
        "* Added internediate dense layer which improved score. This also makes the class much more flexible between \"more features, shorter sequences\" and \"fewer features, longer sequences\"\n",
        "* Changed loss to `BinaryCrossentropy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdNnFFBweh6m",
        "colab": {}
      },
      "source": [
        "def discriminative_model(input_size, hidden_dim): \n",
        "    inputs = tf.keras.Input(shape = input_size)\n",
        "    # normalised1 = LayerNormalization()(inputs1)\n",
        "    GRU_output_sequence, GRU_last_state = tf.keras.layers.GRU(hidden_dim, return_sequences = True, return_state = True)(inputs)\n",
        "    # Dense1 is the y_hat_logit in the original code\n",
        "    Dense1 = tf.keras.layers.Dense(hidden_dim)(GRU_last_state) # Lulu: added intermediate dense layer with increased dimension, scores much better\n",
        "    Dense2 = tf.keras.layers.Dense(1)(Dense1)\n",
        "\n",
        "    # Acti1 is the y_hat in the original code\n",
        "    # It is very odd that the original code seems to compare the result of Dense1 with the one-zero label # Lulu: it's OK, there are losses these types\n",
        "    # while using Acti1 as the prediction result, but it doesn't make sense to me\n",
        "    # I do what I think to be the right thing here - use Acti1 result as the prediction result\n",
        "\n",
        "    Acti1 = tf.keras.layers.Activation(tf.keras.activations.sigmoid)(Dense2)  # Lulu: might not need separate activation layer\n",
        "    \n",
        "    model = tf.keras.Model(inputs = inputs, outputs = [Acti1])\n",
        "    model.compile(optimizer = \"adam\", loss = tf.keras.losses.BinaryCrossentropy()) # Lulu: I think this is a better choice of loss for us\n",
        "    \n",
        "    return model \n",
        "                         \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pR8hvgVzfrE2"
      },
      "source": [
        "# tGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qGLwLGPKiAWB"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nMcNRMf9fxec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fdbc57c3-dbb2-4a53-ec67-e3ab03d226af"
      },
      "source": [
        "ori_data_tgan, gen_data_tgan = load_tGAN()\n",
        "train_data_tgan, val_data_tgan, test_data_tgan, train_labels_tgan, val_labels_tgan, test_labels_tgan = train_val_test_split(ori_data=ori_data_tgan, gen_data=gen_data_tgan)\n",
        "# Check shapes:\n",
        "for array in [train_data_tgan, val_data_tgan, test_data_tgan, train_labels_tgan, val_labels_tgan, test_labels_tgan]:\n",
        "  print(np.shape(array))\n",
        "\n",
        "\n",
        "input_dim, hidden_dim = InputSize(ori_data_tgan)\n",
        "model_tgan = discriminative_model(input_size=input_dim, hidden_dim=hidden_dim)\n",
        "\n",
        "history_model_tgan = model_tgan.fit(train_data_tgan, train_labels_tgan, batch_size=128, epochs=200, validation_data=(val_data_tgan, val_labels_tgan))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9bcd14fe0b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mori_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_data_tgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mori_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_data_tgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check shapes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels_tgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tgan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-06f4d54d0946>\u001b[0m in \u001b[0;36mload_tGAN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_tGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mori_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'synthetic data/TGAN/cat_time_10visits_all_noid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m# shape (12390 patients visits, 10 features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mori_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_removena_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (841 patients, 10 visits, 10 features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File synthetic data/TGAN/cat_time_10visits_all_noid.csv does not exist: 'synthetic data/TGAN/cat_time_10visits_all_noid.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2uzl0PLUiFjo"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FcReew-Fh-oU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09dba430-c9fc-4d75-8b1c-a91ebfff1c93"
      },
      "source": [
        "model_tgan.evaluate(test_data_tgan, test_labels_tgan) # keras built in evaluation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 2ms/step - loss: 0.0078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.007770351134240627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pOlECtD8i2Ay",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b44bf1fd-1b6d-4111-f530-fb2cc82dc104"
      },
      "source": [
        "test_raw_pred_tgan = model_tgan.predict(test_data_tgan)\n",
        "test_pred_tgan = np.round(test_raw_pred_tgan)\n",
        "\n",
        "print(classification_report(test_labels_tgan, test_pred_tgan, digits=5)) # more detailed classification report using sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.98561   1.00000   0.99275       137\n",
            "         1.0    1.00000   0.98276   0.99130       116\n",
            "\n",
            "    accuracy                        0.99209       253\n",
            "   macro avg    0.99281   0.99138   0.99203       253\n",
            "weighted avg    0.99221   0.99209   0.99209       253\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6pejbhz2wYF5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "19a4a621-f8cf-4c9c-ed4f-a38dbed363d5"
      },
      "source": [
        "exp_acc_tgan = np.sum(test_labels_tgan)/np.shape(test_labels_tgan)[0]\n",
        "print('Expected accuracy for an untrained discriminative model = ', str(exp_acc_tgan))\n",
        "print('Final accuracy of trained discriminative model = ', str(accuracy_score(test_labels_tgan, test_pred_tgan)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected accuracy for an untrained discriminative model =  0.45849802371541504\n",
            "Final accuracy of trained discriminative model =  0.9920948616600791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehfU91eGnQj5"
      },
      "source": [
        "# DoppelGANger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SKtwNlQDndJ8"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fm5EvPGGw55i"
      },
      "source": [
        "Lulu: I chose to increase the hidden dimension to 64 because there are only 5 features. This allows the additional dense layer to train from the longer sequences of 130 (compared to length 10 in the tGAN output). Accuracy improved significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mbTC4BndnbRD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3625da58-86b9-45fa-8c47-98f77d520c20"
      },
      "source": [
        "ori_data_dop, gen_data_dop = load_DoppelGANger()\n",
        "train_data_dop, val_data_dop, test_data_dop, train_labels_dop, val_labels_dop, test_labels_dop = train_val_test_split(ori_data=ori_data_dop, gen_data=gen_data_dop)\n",
        "# Check shapes\n",
        "for array in [train_data_dop, val_data_dop, test_data_dop, train_labels_dop, val_labels_dop, test_labels_dop]:\n",
        "  print(np.shape(array))\n",
        "\n",
        "\n",
        "input_dim, hidden_dim = InputSize(ori_data_dop)\n",
        "model_dop = discriminative_model(input_size=input_dim, hidden_dim=64)\n",
        "\n",
        "history_model_dop = model_dop.fit(train_data_dop, train_labels_dop, batch_size=128, epochs=100, validation_data=(val_data_dop, val_labels_dop))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1750, 130, 6)\n",
            "(539, 130, 6)\n",
            "(405, 130, 6)\n",
            "(1750,)\n",
            "(539,)\n",
            "(405,)\n",
            "Epoch 1/100\n",
            "14/14 [==============================] - 2s 149ms/step - loss: 0.6929 - val_loss: 0.6938\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.6932 - val_loss: 0.6931\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 2s 110ms/step - loss: 0.6925 - val_loss: 0.6949\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.6928 - val_loss: 0.6935\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.6921 - val_loss: 0.6926\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.6722 - val_loss: 0.6719\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.6875 - val_loss: 0.6902\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.6877 - val_loss: 0.6840\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.6827 - val_loss: 0.6683\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.6593 - val_loss: 0.6110\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.5793 - val_loss: 0.4968\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.4893 - val_loss: 0.4240\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.4268 - val_loss: 0.2747\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.2535 - val_loss: 0.1146\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.1655 - val_loss: 0.1312\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.2995 - val_loss: 0.2323\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.1865 - val_loss: 0.0910\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.1192 - val_loss: 0.0795\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.1146 - val_loss: 0.0855\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.1129 - val_loss: 0.0973\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.1117 - val_loss: 0.0961\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 0.0994 - val_loss: 0.0943\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0929 - val_loss: 0.0731\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.1046 - val_loss: 0.0970\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.0997 - val_loss: 0.1084\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.1122 - val_loss: 0.1135\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.1153 - val_loss: 0.1136\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.1152 - val_loss: 0.1135\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.1150 - val_loss: 0.1136\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.1152 - val_loss: 0.1137\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.1135 - val_loss: 0.1135\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.1135 - val_loss: 0.1134\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.1135 - val_loss: 0.1082\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.1120 - val_loss: 0.1083\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.1116 - val_loss: 0.1082\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.1118 - val_loss: 0.1082\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 2s 122ms/step - loss: 0.1117 - val_loss: 0.1082\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 2s 127ms/step - loss: 0.1121 - val_loss: 0.1082\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 2s 126ms/step - loss: 0.1119 - val_loss: 0.1083\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.1117 - val_loss: 0.1082\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 2s 129ms/step - loss: 0.1099 - val_loss: 0.1027\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 2s 127ms/step - loss: 0.1083 - val_loss: 0.0748\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0924 - val_loss: 0.0889\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0898 - val_loss: 0.0918\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.1060 - val_loss: 0.1088\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 0.1229 - val_loss: 0.1034\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0869 - val_loss: 0.0701\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0713 - val_loss: 0.0691\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0806 - val_loss: 0.0874\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0813 - val_loss: 0.0912\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0807 - val_loss: 0.0898\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.2050 - val_loss: 0.1775\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 0.3528 - val_loss: 0.9263\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.5505 - val_loss: 0.3602\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.3164 - val_loss: 0.2346\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.1648 - val_loss: 0.1151\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0854 - val_loss: 0.0628\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 0.0551 - val_loss: 0.0423\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.0434 - val_loss: 0.0358\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0404 - val_loss: 0.0331\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.0388 - val_loss: 0.0317\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 0.0375 - val_loss: 0.0303\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 0.0365 - val_loss: 0.0301\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.0345 - val_loss: 0.0342\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 0.0416 - val_loss: 0.0390\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 0.0424 - val_loss: 0.0376\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0410 - val_loss: 0.0374\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0405 - val_loss: 0.0371\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0398 - val_loss: 0.0373\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0396 - val_loss: 0.0376\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 2s 111ms/step - loss: 0.0392 - val_loss: 0.0379\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0390 - val_loss: 0.0380\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0387 - val_loss: 0.0383\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0386 - val_loss: 0.0377\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0386 - val_loss: 0.0383\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0383 - val_loss: 0.0378\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0382 - val_loss: 0.0379\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0371 - val_loss: 0.0388\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0362 - val_loss: 0.0396\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0360 - val_loss: 0.0386\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0356 - val_loss: 0.0371\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0354 - val_loss: 0.0376\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 2s 110ms/step - loss: 0.0353 - val_loss: 0.0375\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0352 - val_loss: 0.0371\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0351 - val_loss: 0.0368\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 0.0352 - val_loss: 0.0370\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 0.0383 - val_loss: 0.0391\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0365 - val_loss: 0.0401\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 0.0341 - val_loss: 0.0348\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0311 - val_loss: 0.0288\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0305 - val_loss: 0.0286\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0302 - val_loss: 0.0326\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0302 - val_loss: 0.0324\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0299 - val_loss: 0.0333\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 2s 111ms/step - loss: 0.0301 - val_loss: 0.0296\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0297 - val_loss: 0.0286\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0296 - val_loss: 0.0289\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0296 - val_loss: 0.0297\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0296 - val_loss: 0.0280\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 0.0297 - val_loss: 0.0272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "81WsXzBboOeF"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nUpYf15oQZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f00f0250-296c-4175-d4c4-5ea8aeaa9647"
      },
      "source": [
        "model_dop.evaluate(test_data_dop, test_labels_dop) # keras built in evaluation"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.028805462643504143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LgZrEVWzoaCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f07885e5-d75f-44d0-8ecb-9f0d412290aa"
      },
      "source": [
        "test_raw_pred_dop = model_dop.predict(test_data_dop)\n",
        "test_pred_dop = np.round(test_raw_pred_dop)\n",
        "\n",
        "report = classification_report(test_labels_dop, test_pred_dop, digits=5, output_dict=True) # more detailed classification report using sklearn\n",
        "report = pd.DataFrame(report).transpose()\n",
        "report.to_csv('discriminative_dop_results_0827am.csv')\n",
        "print(report)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score     support\n",
            "0.0            0.990050  1.000000  0.995000  199.000000\n",
            "1.0            1.000000  0.990291  0.995122  206.000000\n",
            "accuracy       0.995062  0.995062  0.995062    0.995062\n",
            "macro avg      0.995025  0.995146  0.995061  405.000000\n",
            "weighted avg   0.995111  0.995062  0.995062  405.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iVFSzM62uQVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c124b27e-76cf-4195-c253-d99d3e7ab74b"
      },
      "source": [
        "exp_acc_dop = np.sum(test_labels_dop)/np.shape(test_labels_dop)[0]\n",
        "print('Expected accuracy for an untrained discriminative model = ', str(exp_acc_dop))\n",
        "print('Final accuracy of trained discriminative model = ', str(accuracy_score(test_labels_dop, test_pred_dop)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected accuracy for an untrained discriminative model =  0.508641975308642\n",
            "Final accuracy of trained discriminative model =  0.9950617283950617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOTTzeJRteaq",
        "colab_type": "text"
      },
      "source": [
        "We can conclude that the discriminative model can distinguish the synthetic data from the real data very well, so we are expecting further improvements with our synthetic data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2RI844Mtear",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "* <!--[Text](link)-->\n",
        "[Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar. Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf 'Optional title')\n"
      ]
    }
  ]
}