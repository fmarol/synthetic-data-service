{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "discriminative_model_NEW_Lulu.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T5Z25eo1avbL"
      ],
      "authorship_tag": "ABX9TyPhqaiuRL7icBrCb6w76XiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noo-rashbass/synthetic-data-service/blob/master/Evaluation/discriminative_model_NEW_Lulu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PieEO70OaR7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kil-9jsdtDHD",
        "colab_type": "text"
      },
      "source": [
        "Understanding the classification report: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Z25eo1avbL",
        "colab_type": "text"
      },
      "source": [
        "# Functions for Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLS9M8Q1zAzq",
        "colab_type": "text"
      },
      "source": [
        "Lulu's Notes:\n",
        "\n",
        "* separated data loading into functions\n",
        "* returned `hidden_dim` \n",
        "* replaced `mix_divide()` with `train_val_test_split()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMb9XcNja-wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_removena_stack(ori_data):\n",
        "  ori_data = np.split(ori_data, np.shape(ori_data)[0]/10, axis=0)\n",
        "  ori_data_new = []\n",
        "  for array in ori_data:\n",
        "    if not np.isnan(array).any():\n",
        "      ori_data_new.append(array)\n",
        "  return ori_data_new\n",
        "\n",
        "def load_DoppelGANger():\n",
        "  ori_data = np.load('ori_features_prism.npy')\n",
        "  gen_data = np.load('features_600.npy')\n",
        "  return ori_data, gen_data\n",
        "\n",
        "def load_tGAN():\n",
        "\n",
        "  ori_data = pd.read_csv('cat_time_10visits_all_noid.csv').values # shape (12390 patients visits, 10 features)\n",
        "  ori_data = reshape_removena_stack(ori_data) # shape (841 patients, 10 visits, 10 features)\n",
        "\n",
        "  gen_data = np.load('gen_cat_time_10visits_wl_5000it.npy')[:np.shape(ori_data)[0]] # shape (841 patients, 10 visits, 10 features)\n",
        "\n",
        "  return ori_data, gen_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K75wQCZkdyoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MinMaxScaler(data): # This is a normalisation method copied from TGANs code # Lulu: not used\n",
        "  \"\"\"Min Max normalizer.\n",
        "  \n",
        "  Args:\n",
        "    - data: original data\n",
        "  \n",
        "  Returns:\n",
        "    - norm_data: normalized data\n",
        "  \"\"\"\n",
        "  numerator = data - np.min(data, 0)\n",
        "  denominator = np.max(data, 0) - np.min(data, 0)\n",
        "  norm_data = numerator / (denominator + 1e-7)\n",
        "  return norm_data\n",
        "\n",
        "\n",
        "def InputSize(ori_data): # Set the input size to the model\n",
        "    no, seq_len, dim = np.asarray(ori_data).shape \n",
        "    hidden_dim = int(dim/2)\n",
        "    input_dim = [None,dim]\n",
        "    return input_dim, hidden_dim # Lulu: added hidden_dim and renamed input_size because of later conflict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_whodMSUeLix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_test_split(ori_data, gen_data, rate=(0.65, 0.2, 0.15)): # Lulu: using sklearn, replaces mix_divide\n",
        "  # rate = (train, val, test) must sum to one\n",
        "\n",
        "  data = np.concatenate([ori_data,gen_data],axis=0)\n",
        "  labels = np.concatenate([np.ones(len(ori_data)), np.zeros(len(gen_data))], axis=0)\n",
        "\n",
        "  train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=rate[2])\n",
        "  train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, train_size=rate[0]/(rate[0]+rate[1]))\n",
        "  return train_data, val_data, test_data, train_labels, val_labels, test_labels"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY46QrK-edLZ",
        "colab_type": "text"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOYq9ZKxkHd",
        "colab_type": "text"
      },
      "source": [
        "Lulu's Notes:\n",
        "\n",
        "* No normalisation used\n",
        "* Added internediate dense layer which improved score. This also makes the class much more flexible between \"more features, shorter sequences\" and \"fewer features, longer sequences\"\n",
        "* Changed loss to `BinaryCrossentropy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdNnFFBweh6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminative_model(input_size, hidden_dim): \n",
        "    inputs = tf.keras.Input(shape = input_size)\n",
        "    # normalised1 = LayerNormalization()(inputs1)\n",
        "    GRU_output_sequence, GRU_last_state = tf.keras.layers.GRU(hidden_dim, return_sequences = True, return_state = True)(inputs)\n",
        "    # Dense1 is the y_hat_logit in the original code\n",
        "    Dense1 = tf.keras.layers.Dense(hidden_dim)(GRU_last_state) # Lulu: added intermediate dense layer with increased dimension, scores much better\n",
        "    Dense2 = tf.keras.layers.Dense(1)(Dense1)\n",
        "\n",
        "    # Acti1 is the y_hat in the original code\n",
        "    # It is very odd that the original code seems to compare the result of Dense1 with the one-zero label # Lulu: it's OK, there are losses these types\n",
        "    # while using Acti1 as the prediction result, but it doesn't make sense to me\n",
        "    # I do what I think to be the right thing here - use Acti1 result as the prediction result\n",
        "\n",
        "    Acti1 = tf.keras.layers.Activation(tf.keras.activations.sigmoid)(Dense2)  # Lulu: might not need separate activation layer\n",
        "    \n",
        "    model = tf.keras.Model(inputs = inputs, outputs = [Acti1])\n",
        "    model.compile(optimizer = \"adam\", loss = tf.keras.losses.BinaryCrossentropy()) # Lulu: I think this is a better choice of loss for us\n",
        "    \n",
        "    return model \n",
        "                         \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR8hvgVzfrE2",
        "colab_type": "text"
      },
      "source": [
        "# tGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLwLGPKiAWB",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMcNRMf9fxec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47ae0a2a-65b2-4d6a-ca2a-825c002c3ebb"
      },
      "source": [
        "ori_data_tgan, gen_data_tgan = load_tGAN()\n",
        "train_data_tgan, val_data_tgan, test_data_tgan, train_labels_tgan, val_labels_tgan, test_labels_tgan = train_val_test_split(ori_data=ori_data_tgan, gen_data=gen_data_tgan)\n",
        "# Check shapes:\n",
        "for array in [train_data_tgan, val_data_tgan, test_data_tgan, train_labels_tgan, val_labels_tgan, test_labels_tgan]:\n",
        "  print(np.shape(array))\n",
        "\n",
        "\n",
        "input_dim, hidden_dim = InputSize(ori_data_tgan)\n",
        "model_tgan = discriminative_model(input_size=input_dim, hidden_dim=hidden_dim)\n",
        "\n",
        "history_model_tgan = model_tgan.fit(train_data_tgan, train_labels_tgan, batch_size=128, epochs=200, validation_data=(val_data_tgan, val_labels_tgan))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.9691 - val_loss: 0.8615\n",
            "Epoch 2/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8905 - val_loss: 0.7982\n",
            "Epoch 3/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8166 - val_loss: 0.7571\n",
            "Epoch 4/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7622 - val_loss: 0.7270\n",
            "Epoch 5/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7310 - val_loss: 0.7130\n",
            "Epoch 6/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7119 - val_loss: 0.6953\n",
            "Epoch 7/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6895 - val_loss: 0.6718\n",
            "Epoch 8/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6673 - val_loss: 0.6530\n",
            "Epoch 9/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6460 - val_loss: 0.6312\n",
            "Epoch 10/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6193 - val_loss: 0.6165\n",
            "Epoch 11/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5972 - val_loss: 0.6023\n",
            "Epoch 12/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5849 - val_loss: 0.5935\n",
            "Epoch 13/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5754 - val_loss: 0.5883\n",
            "Epoch 14/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.5674 - val_loss: 0.5797\n",
            "Epoch 15/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5589 - val_loss: 0.5740\n",
            "Epoch 16/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5499 - val_loss: 0.5666\n",
            "Epoch 17/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5415 - val_loss: 0.5602\n",
            "Epoch 18/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5334 - val_loss: 0.5527\n",
            "Epoch 19/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5246 - val_loss: 0.5460\n",
            "Epoch 20/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5162 - val_loss: 0.5380\n",
            "Epoch 21/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5071 - val_loss: 0.5299\n",
            "Epoch 22/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4982 - val_loss: 0.5222\n",
            "Epoch 23/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4894 - val_loss: 0.5138\n",
            "Epoch 24/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4797 - val_loss: 0.5053\n",
            "Epoch 25/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4713 - val_loss: 0.4980\n",
            "Epoch 26/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4618 - val_loss: 0.4861\n",
            "Epoch 27/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4516 - val_loss: 0.4777\n",
            "Epoch 28/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4431 - val_loss: 0.4663\n",
            "Epoch 29/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4346 - val_loss: 0.4625\n",
            "Epoch 30/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4222 - val_loss: 0.4486\n",
            "Epoch 31/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4131 - val_loss: 0.4438\n",
            "Epoch 32/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4047 - val_loss: 0.4291\n",
            "Epoch 33/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3941 - val_loss: 0.4225\n",
            "Epoch 34/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3853 - val_loss: 0.4095\n",
            "Epoch 35/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3740 - val_loss: 0.4014\n",
            "Epoch 36/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3637 - val_loss: 0.3913\n",
            "Epoch 37/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3561 - val_loss: 0.3823\n",
            "Epoch 38/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3470 - val_loss: 0.3760\n",
            "Epoch 39/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3368 - val_loss: 0.3658\n",
            "Epoch 40/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3291 - val_loss: 0.3599\n",
            "Epoch 41/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3211 - val_loss: 0.3482\n",
            "Epoch 42/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3125 - val_loss: 0.3452\n",
            "Epoch 43/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3048 - val_loss: 0.3333\n",
            "Epoch 44/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2964 - val_loss: 0.3261\n",
            "Epoch 45/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2886 - val_loss: 0.3158\n",
            "Epoch 46/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2801 - val_loss: 0.3103\n",
            "Epoch 47/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2713 - val_loss: 0.2977\n",
            "Epoch 48/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2620 - val_loss: 0.2884\n",
            "Epoch 49/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2523 - val_loss: 0.2749\n",
            "Epoch 50/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2449 - val_loss: 0.2683\n",
            "Epoch 51/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2329 - val_loss: 0.2516\n",
            "Epoch 52/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2232 - val_loss: 0.2403\n",
            "Epoch 53/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2120 - val_loss: 0.2292\n",
            "Epoch 54/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2030 - val_loss: 0.2240\n",
            "Epoch 55/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1950 - val_loss: 0.2088\n",
            "Epoch 56/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1882 - val_loss: 0.2055\n",
            "Epoch 57/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1794 - val_loss: 0.1966\n",
            "Epoch 58/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1762 - val_loss: 0.1921\n",
            "Epoch 59/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1678 - val_loss: 0.1822\n",
            "Epoch 60/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1629 - val_loss: 0.1793\n",
            "Epoch 61/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1573 - val_loss: 0.1719\n",
            "Epoch 62/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1519 - val_loss: 0.1637\n",
            "Epoch 63/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1477 - val_loss: 0.1635\n",
            "Epoch 64/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1425 - val_loss: 0.1550\n",
            "Epoch 65/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1383 - val_loss: 0.1523\n",
            "Epoch 66/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1342 - val_loss: 0.1467\n",
            "Epoch 67/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1306 - val_loss: 0.1481\n",
            "Epoch 68/200\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1284 - val_loss: 0.1403\n",
            "Epoch 69/200\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1252 - val_loss: 0.1359\n",
            "Epoch 70/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1201 - val_loss: 0.1361\n",
            "Epoch 71/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1169 - val_loss: 0.1289\n",
            "Epoch 72/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1139 - val_loss: 0.1279\n",
            "Epoch 73/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1114 - val_loss: 0.1242\n",
            "Epoch 74/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1085 - val_loss: 0.1218\n",
            "Epoch 75/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1052 - val_loss: 0.1181\n",
            "Epoch 76/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1034 - val_loss: 0.1146\n",
            "Epoch 77/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1002 - val_loss: 0.1151\n",
            "Epoch 78/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0991 - val_loss: 0.1075\n",
            "Epoch 79/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0964 - val_loss: 0.1097\n",
            "Epoch 80/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0933 - val_loss: 0.1049\n",
            "Epoch 81/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0926 - val_loss: 0.1025\n",
            "Epoch 82/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0885 - val_loss: 0.1025\n",
            "Epoch 83/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0858 - val_loss: 0.0995\n",
            "Epoch 84/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0874 - val_loss: 0.0959\n",
            "Epoch 85/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0899 - val_loss: 0.0986\n",
            "Epoch 86/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0915 - val_loss: 0.1005\n",
            "Epoch 87/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0846 - val_loss: 0.0904\n",
            "Epoch 88/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0788 - val_loss: 0.0905\n",
            "Epoch 89/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0738 - val_loss: 0.0887\n",
            "Epoch 90/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0725 - val_loss: 0.0887\n",
            "Epoch 91/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0711 - val_loss: 0.0849\n",
            "Epoch 92/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0865\n",
            "Epoch 93/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0667 - val_loss: 0.0811\n",
            "Epoch 94/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0654 - val_loss: 0.0807\n",
            "Epoch 95/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0658 - val_loss: 0.0809\n",
            "Epoch 96/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0645 - val_loss: 0.0769\n",
            "Epoch 97/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0625 - val_loss: 0.0785\n",
            "Epoch 98/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0606 - val_loss: 0.0762\n",
            "Epoch 99/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0743\n",
            "Epoch 100/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0756\n",
            "Epoch 101/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0752\n",
            "Epoch 102/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0723\n",
            "Epoch 103/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0563 - val_loss: 0.0748\n",
            "Epoch 104/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0549 - val_loss: 0.0712\n",
            "Epoch 105/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 0.0708\n",
            "Epoch 106/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0511 - val_loss: 0.0701\n",
            "Epoch 107/200\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.0507 - val_loss: 0.0672\n",
            "Epoch 108/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0500 - val_loss: 0.0679\n",
            "Epoch 109/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0495 - val_loss: 0.0658\n",
            "Epoch 110/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0482 - val_loss: 0.0687\n",
            "Epoch 111/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 0.0666\n",
            "Epoch 112/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 0.0711\n",
            "Epoch 113/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 0.0677\n",
            "Epoch 114/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 0.0663\n",
            "Epoch 115/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0469 - val_loss: 0.0623\n",
            "Epoch 116/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 0.0612\n",
            "Epoch 117/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0466 - val_loss: 0.0658\n",
            "Epoch 118/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 0.0605\n",
            "Epoch 119/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0457 - val_loss: 0.0630\n",
            "Epoch 120/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0468 - val_loss: 0.0594\n",
            "Epoch 121/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0431 - val_loss: 0.0606\n",
            "Epoch 122/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0426 - val_loss: 0.0590\n",
            "Epoch 123/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0395 - val_loss: 0.0591\n",
            "Epoch 124/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0392 - val_loss: 0.0585\n",
            "Epoch 125/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0384 - val_loss: 0.0580\n",
            "Epoch 126/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0383 - val_loss: 0.0571\n",
            "Epoch 127/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0378 - val_loss: 0.0584\n",
            "Epoch 128/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0378 - val_loss: 0.0569\n",
            "Epoch 129/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0371 - val_loss: 0.0566\n",
            "Epoch 130/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0376 - val_loss: 0.0564\n",
            "Epoch 131/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0367 - val_loss: 0.0553\n",
            "Epoch 132/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0357 - val_loss: 0.0559\n",
            "Epoch 133/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0360 - val_loss: 0.0546\n",
            "Epoch 134/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0359 - val_loss: 0.0581\n",
            "Epoch 135/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0351 - val_loss: 0.0531\n",
            "Epoch 136/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0347 - val_loss: 0.0570\n",
            "Epoch 137/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0343 - val_loss: 0.0527\n",
            "Epoch 138/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0337 - val_loss: 0.0555\n",
            "Epoch 139/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0327 - val_loss: 0.0521\n",
            "Epoch 140/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0330 - val_loss: 0.0505\n",
            "Epoch 141/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0321 - val_loss: 0.0544\n",
            "Epoch 142/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0328 - val_loss: 0.0506\n",
            "Epoch 143/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0316 - val_loss: 0.0520\n",
            "Epoch 144/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0321 - val_loss: 0.0579\n",
            "Epoch 145/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0368 - val_loss: 0.0460\n",
            "Epoch 146/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0317 - val_loss: 0.0653\n",
            "Epoch 147/200\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.0397 - val_loss: 0.0461\n",
            "Epoch 148/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0314 - val_loss: 0.0530\n",
            "Epoch 149/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0308 - val_loss: 0.0455\n",
            "Epoch 150/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0294 - val_loss: 0.0498\n",
            "Epoch 151/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0280 - val_loss: 0.0463\n",
            "Epoch 152/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0279 - val_loss: 0.0532\n",
            "Epoch 153/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0303 - val_loss: 0.0476\n",
            "Epoch 154/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0284 - val_loss: 0.0468\n",
            "Epoch 155/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0276 - val_loss: 0.0460\n",
            "Epoch 156/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0265 - val_loss: 0.0463\n",
            "Epoch 157/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0262 - val_loss: 0.0482\n",
            "Epoch 158/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0258 - val_loss: 0.0457\n",
            "Epoch 159/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0439\n",
            "Epoch 160/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0258 - val_loss: 0.0444\n",
            "Epoch 161/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0268 - val_loss: 0.0502\n",
            "Epoch 162/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0265 - val_loss: 0.0425\n",
            "Epoch 163/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0443\n",
            "Epoch 164/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0491\n",
            "Epoch 165/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0415\n",
            "Epoch 166/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0240 - val_loss: 0.0445\n",
            "Epoch 167/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0235 - val_loss: 0.0429\n",
            "Epoch 168/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0456\n",
            "Epoch 169/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0241 - val_loss: 0.0417\n",
            "Epoch 170/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0226 - val_loss: 0.0437\n",
            "Epoch 171/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0448\n",
            "Epoch 172/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0419\n",
            "Epoch 173/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0219 - val_loss: 0.0433\n",
            "Epoch 174/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0414\n",
            "Epoch 175/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0241 - val_loss: 0.0435\n",
            "Epoch 176/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0220 - val_loss: 0.0456\n",
            "Epoch 177/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0235 - val_loss: 0.0402\n",
            "Epoch 178/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0217 - val_loss: 0.0419\n",
            "Epoch 179/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0208 - val_loss: 0.0411\n",
            "Epoch 180/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0211 - val_loss: 0.0414\n",
            "Epoch 181/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0207 - val_loss: 0.0434\n",
            "Epoch 182/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0442\n",
            "Epoch 183/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0502\n",
            "Epoch 184/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0421\n",
            "Epoch 185/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0206 - val_loss: 0.0378\n",
            "Epoch 186/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0211 - val_loss: 0.0446\n",
            "Epoch 187/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0419\n",
            "Epoch 188/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0212 - val_loss: 0.0438\n",
            "Epoch 189/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0203 - val_loss: 0.0379\n",
            "Epoch 190/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0191 - val_loss: 0.0404\n",
            "Epoch 191/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0189 - val_loss: 0.0436\n",
            "Epoch 192/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0201 - val_loss: 0.0413\n",
            "Epoch 193/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0185 - val_loss: 0.0364\n",
            "Epoch 194/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0192 - val_loss: 0.0400\n",
            "Epoch 195/200\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.0223 - val_loss: 0.0596\n",
            "Epoch 196/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0368 - val_loss: 0.0476\n",
            "Epoch 197/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0235 - val_loss: 0.0442\n",
            "Epoch 198/200\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0204 - val_loss: 0.0459\n",
            "Epoch 199/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0192 - val_loss: 0.0363\n",
            "Epoch 200/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uzl0PLUiFjo",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcReew-Fh-oU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09dba430-c9fc-4d75-8b1c-a91ebfff1c93"
      },
      "source": [
        "model_tgan.evaluate(test_data_tgan, test_labels_tgan) # keras built in evaluation"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 2ms/step - loss: 0.0078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.007770351134240627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOlECtD8i2Ay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b44bf1fd-1b6d-4111-f530-fb2cc82dc104"
      },
      "source": [
        "test_raw_pred_tgan = model_tgan.predict(test_data_tgan)\n",
        "test_pred_tgan = np.round(test_raw_pred_tgan)\n",
        "\n",
        "print(classification_report(test_labels_tgan, test_pred_tgan, digits=5)) # more detailed classification report using sklearn"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.98561   1.00000   0.99275       137\n",
            "         1.0    1.00000   0.98276   0.99130       116\n",
            "\n",
            "    accuracy                        0.99209       253\n",
            "   macro avg    0.99281   0.99138   0.99203       253\n",
            "weighted avg    0.99221   0.99209   0.99209       253\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pejbhz2wYF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "19a4a621-f8cf-4c9c-ed4f-a38dbed363d5"
      },
      "source": [
        "exp_acc_tgan = np.sum(test_labels_tgan)/np.shape(test_labels_tgan)[0]\n",
        "print('Expected accuracy for an untrained discriminative model = ', str(exp_acc_tgan))\n",
        "print('Final accuracy of trained discriminative model = ', str(accuracy_score(test_labels_tgan, test_pred_tgan)))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected accuracy for an untrained discriminative model =  0.45849802371541504\n",
            "Final accuracy of trained discriminative model =  0.9920948616600791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehfU91eGnQj5",
        "colab_type": "text"
      },
      "source": [
        "# DoppelGANger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKtwNlQDndJ8",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm5EvPGGw55i",
        "colab_type": "text"
      },
      "source": [
        "Lulu: I chose to increase the hidden dimension to 64 because there are only 5 features. This allows the additional dense layer to train from the longer sequences of 130 (compared to length 10 in the tGAN output). Accuracy improved significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbTC4BndnbRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fd49080-37c2-4648-e10c-2946b7a7d2c0"
      },
      "source": [
        "ori_data_dop, gen_data_dop = load_DoppelGANger()\n",
        "train_data_dop, val_data_dop, test_data_dop, train_labels_dop, val_labels_dop, test_labels_dop = train_val_test_split(ori_data=ori_data_dop, gen_data=gen_data_dop)\n",
        "# Check shapes\n",
        "for array in [train_data_dop, val_data_dop, test_data_dop, train_labels_dop, val_labels_dop, test_labels_dop]:\n",
        "  print(np.shape(array))\n",
        "\n",
        "\n",
        "input_dim, hidden_dim = InputSize(ori_data_dop)\n",
        "model_dop = discriminative_model(input_size=input_dim, hidden_dim=64)\n",
        "\n",
        "history_model_dop = model_dop.fit(train_data_dop, train_labels_dop, batch_size=128, epochs=100, validation_data=(val_data_dop, val_labels_dop))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1750, 130, 5)\n",
            "(539, 130, 5)\n",
            "(405, 130, 5)\n",
            "(1750,)\n",
            "(539,)\n",
            "(405,)\n",
            "Epoch 1/100\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.6932 - val_loss: 0.6936\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.6927 - val_loss: 0.6935\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 1s 95ms/step - loss: 0.6930 - val_loss: 0.6945\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 1s 95ms/step - loss: 0.6923 - val_loss: 0.6935\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 1s 105ms/step - loss: 0.6926 - val_loss: 0.6945\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 1s 105ms/step - loss: 0.6920 - val_loss: 0.6947\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 1s 95ms/step - loss: 0.6917 - val_loss: 0.6957\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 1s 95ms/step - loss: 0.6918 - val_loss: 0.6948\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.6923 - val_loss: 0.6959\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.6920 - val_loss: 0.6936\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.6919 - val_loss: 0.6939\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6913 - val_loss: 0.6961\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6919 - val_loss: 0.6974\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6904 - val_loss: 0.6960\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6991 - val_loss: 0.6197\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6872 - val_loss: 0.6951\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6927 - val_loss: 0.6934\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6924 - val_loss: 0.6936\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.6924 - val_loss: 0.6939\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6926 - val_loss: 0.6943\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 1s 104ms/step - loss: 0.6927 - val_loss: 0.6935\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6922 - val_loss: 0.6937\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6921 - val_loss: 0.6939\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6922 - val_loss: 0.6943\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 2s 107ms/step - loss: 0.6921 - val_loss: 0.6940\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.6923 - val_loss: 0.6936\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6919 - val_loss: 0.6942\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6919 - val_loss: 0.6938\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.6918 - val_loss: 0.6939\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6918 - val_loss: 0.6938\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.6918 - val_loss: 0.6943\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.6917 - val_loss: 0.6939\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.6918 - val_loss: 0.6944\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6917 - val_loss: 0.6943\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6918 - val_loss: 0.6933\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6915 - val_loss: 0.6945\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 1s 95ms/step - loss: 0.6923 - val_loss: 0.6948\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.6913 - val_loss: 0.6936\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.6912 - val_loss: 0.6950\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.6910 - val_loss: 0.6951\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6912 - val_loss: 0.6950\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.6910 - val_loss: 0.6965\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 1s 94ms/step - loss: 0.6907 - val_loss: 0.6952\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6904 - val_loss: 0.6953\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.6902 - val_loss: 0.6960\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.6900 - val_loss: 0.6966\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 1s 104ms/step - loss: 0.6893 - val_loss: 0.6973\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.6834 - val_loss: 0.6621\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.5960 - val_loss: 0.4739\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 1s 105ms/step - loss: 0.3995 - val_loss: 0.3201\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.2781 - val_loss: 0.4244\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.2122 - val_loss: 0.1716\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.1459 - val_loss: 0.2088\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 1s 103ms/step - loss: 0.1074 - val_loss: 0.1132\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.1329 - val_loss: 0.2942\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.1474 - val_loss: 0.1706\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 1s 97ms/step - loss: 0.0753 - val_loss: 0.1223\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 0.0801 - val_loss: 0.2045\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0814 - val_loss: 0.1049\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0582 - val_loss: 0.0724\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0485 - val_loss: 0.1052\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0453 - val_loss: 0.1009\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.0478 - val_loss: 0.0804\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.0341 - val_loss: 0.0807\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.0467 - val_loss: 0.0908\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0312 - val_loss: 0.0816\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 1s 105ms/step - loss: 0.0325 - val_loss: 0.2118\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.0997 - val_loss: 0.1291\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 1s 104ms/step - loss: 0.0598 - val_loss: 0.0785\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0376 - val_loss: 0.0833\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0351 - val_loss: 0.0959\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0296 - val_loss: 0.0817\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.0454 - val_loss: 0.1246\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 1s 104ms/step - loss: 0.0385 - val_loss: 0.0776\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0284 - val_loss: 0.0600\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0178 - val_loss: 0.0758\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0290 - val_loss: 0.0685\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0256 - val_loss: 0.0572\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 2s 107ms/step - loss: 0.0193 - val_loss: 0.0692\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0124 - val_loss: 0.0606\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0106 - val_loss: 0.0740\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0095 - val_loss: 0.0574\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 1s 104ms/step - loss: 0.0261 - val_loss: 0.2947\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.1963 - val_loss: 0.1540\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0844 - val_loss: 0.1173\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0627 - val_loss: 0.1118\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0477 - val_loss: 0.0917\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 1s 103ms/step - loss: 0.0346 - val_loss: 0.0766\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0246 - val_loss: 0.0867\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.0180 - val_loss: 0.1266\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0176 - val_loss: 0.1757\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0143 - val_loss: 0.0647\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0162 - val_loss: 0.0879\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 1s 102ms/step - loss: 0.0174 - val_loss: 0.0811\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 1s 101ms/step - loss: 0.0136 - val_loss: 0.0731\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0101 - val_loss: 0.0670\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 1s 106ms/step - loss: 0.0109 - val_loss: 0.0660\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 1s 98ms/step - loss: 0.0098 - val_loss: 0.0743\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 1s 99ms/step - loss: 0.0487 - val_loss: 0.6924\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 1s 100ms/step - loss: 0.2732 - val_loss: 0.1754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81WsXzBboOeF",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nUpYf15oQZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "416aebd0-493f-49d2-dbed-6043ef25f1d0"
      },
      "source": [
        "model_dop.evaluate(test_data_dop, test_labels_dop) # keras built in evaluation"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 11ms/step - loss: 0.1168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11682559549808502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgZrEVWzoaCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "2c565486-5bfd-4049-827a-da01110f9931"
      },
      "source": [
        "test_raw_pred_dop = model_dop.predict(test_data_dop)\n",
        "test_pred_dop = np.round(test_raw_pred_dop)\n",
        "\n",
        "print(classification_report(test_labels_dop, test_pred_dop, digits=5)) # more detailed classification report using sklearn"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    1.00000   0.90674   0.95109       193\n",
            "         1.0    0.92174   1.00000   0.95928       212\n",
            "\n",
            "    accuracy                        0.95556       405\n",
            "   macro avg    0.96087   0.95337   0.95518       405\n",
            "weighted avg    0.95903   0.95556   0.95537       405\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVFSzM62uQVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5cc3c50e-5c89-4f21-e5d3-9e29cef5594f"
      },
      "source": [
        "exp_acc_dop = np.sum(test_labels_dop)/np.shape(test_labels_dop)[0]\n",
        "print('Expected accuracy for an untrained discriminative model = ', str(exp_acc_dop))\n",
        "print('Final accuracy of trained discriminative model = ', str(accuracy_score(test_labels_dop, test_pred_dop)))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected accuracy for an untrained discriminative model =  0.5234567901234568\n",
            "Final accuracy of trained discriminative model =  0.9555555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}