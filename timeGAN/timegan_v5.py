import tensorflow as tf 
print(tf.__version__)
import numpy as np
from utils3 import extract_time, rnn_cell, random_generator, batch_generator

def timegan(ori_data, parameters):
    
    #reset default graph?

    # Basic Parameters
    no, seq_len, dim = np.asarray(ori_data).shape
    
    # Maximum sequence length and each sequence length
    ori_time, max_seq_len = extract_time(ori_data)
  
    def MinMaxScaler(data):
        """Min-Max Normalizer.
        
        Args:
        - data: raw data
        
        Returns:
        - norm_data: normalized data
        - min_val: minimum values (for renormalization)
        - max_val: maximum values (for renormalization)
        """    
        min_val = np.min(np.min(data, axis = 0), axis = 0)
        data = data - min_val
        
        max_val = np.max(np.max(data, axis = 0), axis = 0)
        norm_data = data / (max_val + 1e-7)
        
        return norm_data, min_val, max_val
  
    # Normalization
    ori_data, min_val, max_val = MinMaxScaler(ori_data)
              
    ## Build a RNN networks          
  
    # Network Parameters
    hidden_dim   = parameters['hidden_dim'] 
    num_layers   = parameters['num_layer']
    iterations   = parameters['iterations']
    batch_size   = parameters['batch_size']
    module_name  = parameters['module'] 
    z_dim        = dim
    gamma        = 1

    # Input place holders
    # tf.keras.backend.placeholder
    #tf.Variable
    #https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2
    X = tf.keras.Input(shape=(max_seq_len, dim), dtype="float32", name="myinput_x")
    Z = tf.keras.Input(shape=(max_seq_len, dim), dtype="float32", name="myinput_z")
    T = tf.keras.Input(shape=(), dtype="int32", name="myinput_t")

    def make_embedder ():
        """Embedding network between original feature space to latent space.
        
        Args:
        - X: input time-series features
        - T: input time information
        
        Returns:
        - H: embeddings
        """
        embedder_model = tf.keras.Sequential(name='embedder')
        for i in range(num_layers):
            embedder_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        embedder_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        return embedder_model

    def make_recovery ():   
        """Recovery network from latent space to original space.
        
        Args:
        - H: latent representation
        - T: input time information
        
        Returns:
        - X_tilde: recovered data
        """     
        recovery_model = tf.keras.Sequential(name='recovery')
        for i in range(num_layers):
            recovery_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        recovery_model.add(tf.keras.layers.Dense(dim, activation='sigmoid'))

        return recovery_model
  
    def make_generator ():  
        """Generator function: Generate time-series data in latent space.
        
        Args:
        - Z: random variables
        - T: input time information
        
        Returns:
        - E: generated embedding
        """ 
        generator_model = tf.keras.Sequential(name='generator')
        for i in range(num_layers):
            generator_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        generator_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        return generator_model

    def make_supervisor (): 
        """Generate next sequence using the previous sequence.
        
        Args:
        - H: latent representation
        - T: input time information
        
        Returns:
        - S: generated sequence based on the latent representations generated by the generator
        """     
        supervisor_model = tf.keras.Sequential(name='supervisor')
        for i in range(num_layers-1):
            supervisor_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        supervisor_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        return supervisor_model


    embedder_model = make_embedder()
    recovery_model = make_recovery()
    generator_model = make_generator()
    supervisor_model = make_supervisor()


    def get_embedder_0_loss(X, X_tilde):
        mse = tf.keras.losses.MeanSquaredError() 

        E_loss_T0 = mse(X, X_tilde) ##reconstruction loss
        E_loss0 = 10*tf.sqrt(E_loss_T0)
        return E_loss0

    def get_generator_s_loss(H, H_hat_supervise):
        mse = tf.keras.losses.MeanSquaredError()
        G_loss_S = mse(H[:,1:,:], H_hat_supervise[:,:-1,:])
        return G_loss_S

    # optimizer
    embedder_optimizer = tf.keras.optimizers.Adam()
    gen_s_optimizer = tf.keras.optimizers.Adam()

    @tf.function
    def train_step_embedder(X_mb, T_mb):

        with tf.GradientTape() as embedder_tape:
            H_mb = embedder_model(X_mb)
            X_tilde_mb = recovery_model(H_mb)

            embedder_0_loss = get_embedder_0_loss(X_mb, X_tilde_mb)
            gradients_of_embedder = embedder_tape.gradient(embedder_0_loss, embedder_model.trainable_variables)
            embedder_optimizer.apply_gradients(zip(gradients_of_embedder, embedder_model.trainable_variables))
        
        return H_mb

    def train_step_generator_s(Z_mb, T_mb, H_mb):
        
        with tf.GradientTape() as gen_s_tape: #, tf.GradientTape() as s_tape:
            E_hat_mb = generator_model(Z_mb, T_mb)
            H_hat_mb = supervisor_model(E_hat_mb, T_mb)
            H_hat_supervise_mb = supervisor_model(H_mb, T_mb)

            gen_s_loss = get_generator_s_loss(H_mb, H_hat_supervise_mb) #hot sure if i shoudl do whole gen loss or only gen_s loss
            vars = generator_model.trainable_variables + supervisor_model.trainable_variables
            #vars = [generator_model.trainable_variables, supervisor_model.trainable_variables]
            gradients_of_gen_s = gen_s_tape.gradient(gen_s_loss, vars)
            gen_s_optimizer.apply_gradients(zip(gradients_of_gen_s, vars))

    
    def train():
        print('Start Embedding Network Training')

        for itt in range(iterations):
            X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size) 
            H_mb = train_step_embedder(X_mb, T_mb)


        print('Finish Embedding Network Training')

        print('Start Training with Supervised Loss Only')

        for itt in range(iterations):
            for kk in range(2):
                X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size) 
                Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
                train_step_generator_s(Z_mb, T_mb, H_mb)

        print('Finish Training with Supervised Loss Only')

    train()

####TESTING####

from data_loading import real_data_loading, sine_data_generation

data_name = 'sine'
seq_len = 5

if data_name in ['stock', 'energy']:
  ori_data = real_data_loading(data_name, seq_len)
elif data_name == 'sine':
  # Set number of samples and its dimensions
  no, dim = 20, 2
  ori_data = sine_data_generation(no, seq_len, dim)
    
print(data_name + ' dataset is ready.')

## Newtork parameters
parameters = dict()

parameters['module'] = 'gru' 
parameters['hidden_dim'] = 6
parameters['num_layer'] = 3
parameters['iterations'] = 10
parameters['batch_size'] = 4

timegan(ori_data, parameters)