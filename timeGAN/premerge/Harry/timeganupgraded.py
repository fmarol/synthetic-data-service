"""Time-series Generative Adversarial Networks (TimeGAN) Codebase.

Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, 
"Time-series Generative Adversarial Networks," 
Neural Information Processing Systems (NeurIPS), 2019.

Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks

Last updated Date: April 24th 2020
Code author: Jinsung Yoon (jsyoon0823@gmail.com)

-----------------------------

timegan.py

Note: Use original data as training set to generater synthetic data (time-series)
"""




# Necessary Packages
import tensorflow as tf 

if tf.test.gpu_device_name(): 

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

   print("Please install GPU version of TF")
import numpy as np
from utils import extract_time, rnn_cell, random_generator, batch_generator
import warnings
import sys
warnings.filterwarnings("ignore")

def timegan (ori_data, parameters):

  """TimeGAN function.
  
  Use original data as training set to generater synthetic data (time-series)
  
  Args:
    - ori_data: original time-series data
    - parameters: TimeGAN network parameters
    
  Returns:
    - generated_data: generated time-series data
  """
  

  # Basic Parameters
  no, seq_len, dim = np.asarray(ori_data).shape

 # Maximum sequence length and each sequence length
  ori_time, max_seq_len = extract_time(ori_data)

  def MinMaxScaler(data):
    """Min-Max Normalizer.
    
    Args:
      - data: raw data
      
    Returns:
      - norm_data: normalized data
      - min_val: minimum values (for renormalization)
      - max_val: maximum values (for renormalization)
    """    
    min_val = np.min(np.min(data, axis = 0), axis = 0)
    data = data - min_val
      
    max_val = np.max(np.max(data, axis = 0), axis = 0)
    norm_data = data / (max_val + 1e-7)
      
    return norm_data, min_val, max_val
  
  # Normalization
  ori_data, min_val, max_val = MinMaxScaler(ori_data)
              
  ## Build a RNN networks          
  
  # Network Parameters
  hidden_dim   = parameters['hidden_dim'] 
  num_layers   = parameters['num_layer']
  iterations   = parameters['iterations']
  batch_size   = parameters['batch_size']
  module_name  = parameters['module'] 
  z_dim        = dim
  gamma        = 1
    
  def embedder ():
    """Embedding network between original feature space to latent space.
    
    Args:
      - X: input time-series features
      - T: input time information
      
    Returns:
      - H: embeddings
    """
    e_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.GRUCell(hidden_dim, activation = tf.nn.tanh, input_shape=(seq_len, hidden_dim)) for _ in range(num_layers-1)])
    model = tf.keras.Sequential([

            rnn_cell(module_name, hidden_dim, return_sequences=True, input_shape=(seq_len,dim)),
                                   
            tf.keras.layers.RNN(e_cell, return_sequences=True), 
                       
            tf.keras.layers.Dense(hidden_dim, activation=tf.nn.sigmoid)

          ])

    return model
      
  def recovery ():   
    """Recovery network from latent space to original space.
    
    Args:
      - H: latent representation
      - T: input time information
      
    Returns:
      - X_tilde: recovered data
    """     
    r_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.GRUCell(hidden_dim, activation = tf.nn.tanh, input_shape=(seq_len, hidden_dim)) for _ in range(num_layers)])
    model = tf.keras.Sequential([
            
                        
            tf.keras.layers.RNN(r_cell, return_sequences = True),
                       
            tf.keras.layers.Dense(dim, activation=tf.nn.sigmoid)

          ])

    return model
    
  def generator():  
    """Generator function: Generate time-series data in latent space.
    
    Args:
      - Z: random variables
      - T: input time information
      
    Returns:
      - E: generated embedding
    """        
    e_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.GRUCell(hidden_dim, activation = tf.nn.tanh) for _ in range(num_layers)])
    model = tf.keras.Sequential([
                                    
            tf.keras.layers.RNN(e_cell, return_sequences=True),
                       
            tf.keras.layers.Dense(hidden_dim, activation=tf.nn.sigmoid)

          ])

    return model
      
  def supervisor (): 
    """Generate next sequence using the previous sequence.
    
    Args:
      - H: latent representation
      - T: input time information
      
    Returns:
      - S: generated sequence based on the latent representations generated by the generator
    """          
    e_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.GRUCell(hidden_dim, activation = tf.nn.tanh, input_shape=(seq_len, hidden_dim)) for _ in range(num_layers-1)])
    model = tf.keras.Sequential([
                       
            tf.keras.layers.RNN(e_cell, return_sequences=True),
                       
            tf.keras.layers.Dense(hidden_dim, activation=tf.nn.sigmoid)

          ])

    return model

  #sequential may not work        
  def discriminator ():
    """Discriminate the original and synthetic time-series data.
    
    Args:
      - H: latent representation
      - T: input time information
      
    Returns:
      - Y_hat: classification results between original and synthetic time-series
    """        
    d_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.GRUCell(hidden_dim, activation = tf.nn.tanh) for _ in range(num_layers)])
    model = tf.keras.Sequential([
                       
            tf.keras.layers.RNN(d_cell),
                       
            tf.keras.layers.Dense(1, activation=None)

          ])

    return model 
    
  # Embedder & Recovery
  embedder = embedder()
  recovery = recovery()
  # Generator
  generator = generator()
  supervisor = supervisor()
    
  # Discriminator
  discriminator = discriminator()
 
    

            
  # Embedder network loss
  def embed_obj(X, X_tilde):
    return 10*tf.sqrt(tf.compat.v1.losses.mean_squared_error(X, X_tilde))
  def E_loss_T0(X, X_tilde):
    return tf.compat.v1.losses.mean_squared_error(X, X_tilde)
  def E_loss(E_loss0, G_loss_S):
    return E_loss0  + 0.1*G_loss_S

  #Supervised loss
  def supervised_obj(H, H_hat_supervise):
    return tf.compat.v1.losses.mean_squared_error(H[:,1:,:], H_hat_supervise[:,:-1,:])

  #Generator loss
  def generator_obj(G_loss_U, G_loss_U_e, G_loss_S, G_loss_V):
    return G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V 
  
  #Discriminator loss
  def discriminator_obj(Y_real, Y_fake, Y_fake_e):
    D_loss_real = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)
    D_loss_fake = tf.compat.v1.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)
    D_loss_fake_e = tf.compat.v1.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)
    return D_loss_real + D_loss_fake + gamma * D_loss_fake_e
    
  # optimizer
  EO_solver = tf.keras.optimizers.Adam()
  GS_solver = tf.keras.optimizers.Adam() 
  G_solver = tf.keras.optimizers.Adam()
  E_solver = tf.keras.optimizers.Adam()
  D_solver = tf.keras.optimizers.Adam()


  # Train embedder
  @tf.function
  def train_step_embedder(X_mb):
    with tf.GradientTape() as embed_tape:
      embed = embedder(X_mb)
      recover = recovery(embed)
      loss = embed_obj(X_mb, recover)  
      gradients_embed = embed_tape.gradient(loss, embedder.trainable_variables + recovery.trainable_variables)
      EO_solver.apply_gradients(zip(gradients_embed, embedder.trainable_variables + recovery.trainable_variables))
      step_e_loss = E_loss_T0(X_mb, recover)
      return step_e_loss

  #Tensorflow 1 code was confusing so this is likely to be wrong.
  @tf.function
  def train_step_supervised(X_mb, Z_mb):
    with tf.GradientTape() as supervised_tape:
      embed = embedder(X_mb)

      generated = generator(Z_mb)
      supervised_g = supervisor(generated)
      supervised_e = supervisor(embed)
      
      
      loss = supervised_obj(embed, supervised_e)

      gradients_supervised = supervised_tape.gradient(loss, supervisor.trainable_variables + generator.trainable_variables)  
      GS_solver.apply_gradients(zip(gradients_supervised, supervisor.trainable_variables + generator.trainable_variables))

      
      return loss

  

  #Generator is supposed to take it's own previous generated data as input.
  @tf.function
  def train_step_generator(X_mb, Z_mb):
    with tf.GradientTape() as gen_tape:
      
      ##Not sure about feeding the generator embedded
      embed = embedder(X_mb)
      generated = generator(Z_mb)
      supervised_g = supervisor(generated)
      supervised_e = supervisor(embed)

      synth_data = recovery(supervised_g)                        
      
      Y_fake_mb = discriminator(supervised_g)
      Y_real_mb = discriminator(embed)                          
      Y_fake_e_mb = discriminator(generated)

      

           
      G_loss_U = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_mb), Y_fake_mb)
      G_loss_U_e = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e_mb), Y_fake_e_mb)
      G_loss_S = supervised_obj(embed, supervised_e)

      G_loss_V1 = G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(synth_data,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X_mb,[0])[1] + 1e-6)))
      G_loss_V2 = G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(synth_data,[0])[0]) - (tf.nn.moments(X_mb,[0])[0])))
      G_loss_V = G_loss_V1 + G_loss_V2
      
     
      
      loss = generator_obj(G_loss_U, G_loss_U_e, G_loss_S, G_loss_V)

      gradients_generator = gen_tape.gradient(loss, generator.trainable_variables + supervisor.trainable_variables)  
      GS_solver.apply_gradients(zip(gradients_generator, generator.trainable_variables + supervisor.trainable_variables))
      

      return G_loss_U, G_loss_S, G_loss_V
  @tf.function
  def train_step_joint_embed(X_mb, Z_mb):
    with tf.GradientTape() as embed_tape:
      embed = embedder(X_mb)
      supervised = supervisor(embed)
           
      G_loss_S = supervised_obj(embed, supervised)

      
      recover = recovery(embed)
      E_loss0 = embed_obj(X_mb, recover)
      
      loss = E_loss0  + 0.1*G_loss_S

      gradients_embed = embed_tape.gradient(loss, embedder.trainable_variables + recovery.trainable_variables)  
      GS_solver.apply_gradients(zip(gradients_embed, embedder.trainable_variables + recovery.trainable_variables))

      return tf.losses.mean_squared_error(X_mb, recover)


  @tf.function
  def train_step_discriminator(X_mb, Z_mb):
    with tf.GradientTape() as discrim_tape:
      # Check discriminator loss before updating
      embed = embedder(X_mb)
      generated = generator(Z_mb)
      supervised_g = supervisor(generated)
      
      
      Y_fake = discriminator(supervised_g)
      Y_real = discriminator(embed)                          
      Y_fake_e = discriminator(generated)
     
          
      loss = discriminator_obj(Y_real, Y_fake, Y_fake_e)
      if (loss > 0.15):
        gradients_discrim = discrim_tape.gradient(loss, discriminator.trainable_variables)  
        GS_solver.apply_gradients(zip(gradients_discrim, discriminator.trainable_variables))
      return loss      
    
 ## TimeGAN training   
    
  # 1. Embedding network training
  print('Start Embedding Network Training')
      
  def train():
    for itt in range(iterations):
      pass
      # Set mini-batch

      #X_mb = ori_data

      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
      X_mb = X_mb.reshape(batch_size, seq_len, dim)

          
      step_e_loss = train_step_embedder(X_mb)

      # Checkpoint
      if itt % 1000 == 0:
        print('step: '+ str(itt) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss),4))) 
    print('Finish Embedding Network Training')

    # 2. Training only with supervised loss
    print('Start Training with Supervised Loss Only')
    
  
    for itt in range(iterations): 
      # Set mini-batch
      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)    
      # Random vector generation   
      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
      # Train generator       
      step_g_loss_s = train_step_supervised(X_mb, Z_mb)     
      # Checkpoint
      if itt % 1000 == 0:
        print('step: '+ str(itt)  + '/' + str(iterations) +', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s),4)) )

    print('Finish Training with Supervised Loss Only')
    
    # 3. Joint Training
    print('Start Joint Training')
    
    for itt in range(iterations):
      # Generator training (twice more than discriminator training)
      for kk in range(2):
        # Set mini-batch
        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)               
        # Random vector generation
        Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
        # Train generator
        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_step_generator(X_mb, Z_mb)
        #_, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})
        # Train embedder        
        step_e_loss_t0 = train_step_joint_embed(X_mb, Z_mb)

        #_, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})   
             
      # Discriminator training        
      # Set mini-batch
      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           
      # Random vector generation
      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
      
      
      step_d_loss = train_step_discriminator(X_mb, Z_mb)
          
      # Print multiple checkpoints
      if itt % 1000 == 0:
        print('step: '+ str(itt) + '/' + str(iterations) + 
              ', d_loss: ' + str(np.round(step_d_loss,4)) + 
              ', g_loss_u: ' + str(np.round(step_g_loss_u,4)) + 
              ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s),4)) + 
              ', g_loss_v: ' + str(np.round(step_g_loss_v,4)) + 
              ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0),4))  )
    print('Finish Joint Training')

 ## Synthetic data generation
    Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)

    generated = generator(Z_mb)
    supervised = supervisor(generated)
    generated_data_curr = recovery(supervised)
    
    generated_data = list()

    for i in range(no):
        temp = generated_data_curr[i,:ori_time[i],:]
        generated_data.append(temp)
            
    # Renormalization
    generated_data = generated_data * max_val
    generated_data = generated_data + min_val

    return generated_data
    
    
    
    
  return train()
      
   
      
    
##  ## Synthetic data generation
##  Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)
##
##  generator = (ori_data)
##  supervisor = H_hat(generator)
##  generated_data_curr = X_hat(supervisor) sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time})    
##    
##  generated_data = list()
##    
##  for i in range(no):
##    temp = generated_data_curr[i,:ori_time[i],:]
##    generated_data.append(temp)
##        
##  # Renormalization
##  generated_data = generated_data * max_val
##  generated_data = generated_data + min_val
##    
##  return generated_data


