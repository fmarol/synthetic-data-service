import tensorflow as tf 
print(tf.__version__)
import numpy as np
from utils3 import extract_time, rnn_cell, random_generator, batch_generator

def timegan(ori_data, parameters):
    
    #reset default graph?

    # Basic Parameters
    no, seq_len, dim = np.asarray(ori_data).shape
    
    # Maximum sequence length and each sequence length
    ori_time, max_seq_len = extract_time(ori_data)
  
    def MinMaxScaler(data):
        """Min-Max Normalizer.
        
        Args:
        - data: raw data
        
        Returns:
        - norm_data: normalized data
        - min_val: minimum values (for renormalization)
        - max_val: maximum values (for renormalization)
        """    
        min_val = np.min(np.min(data, axis = 0), axis = 0)
        data = data - min_val
        
        max_val = np.max(np.max(data, axis = 0), axis = 0)
        norm_data = data / (max_val + 1e-7)
        
        return norm_data, min_val, max_val
  
    # Normalization
    ori_data, min_val, max_val = MinMaxScaler(ori_data)
              
    ## Build a RNN networks          
  
    # Network Parameters
    hidden_dim   = parameters['hidden_dim'] 
    num_layers   = parameters['num_layer']
    iterations   = parameters['iterations']
    batch_size   = parameters['batch_size']
    module_name  = parameters['module'] 
    z_dim        = dim
    gamma        = 1

    # Input place holders
    # tf.keras.backend.placeholder
    #tf.Variable
    #https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2
    X = tf.keras.Input(shape=(max_seq_len, dim), dtype="float32", name="myinput_x")
    Z = tf.keras.Input(shape=(max_seq_len, dim), dtype="float32", name="myinput_z")
    T = tf.keras.Input(shape=(), dtype="int32", name="myinput_t")

    def embedder (X, T):
        """Embedding network between original feature space to latent space.
    
        Args:
        - X: input time-series features
        - T: input time information
      
        Returns:
        - H: embeddings
        """
        #https://www.datacamp.com/community/tutorials/ten-important-updates-tensorflow
        #https://github.com/tensorflow/tensorflow/issues/28216
        #embedder_model = tf.keras.Sequential([
            #cells = [rnn_cell(module_name, hidden_dim) for _ in range(num_layers)],
            #rnn = tf.keras.layers.StackedRNNCells(cells)
            
            #rnn_cell(module_name, hidden_dim, return_sequences=True),
            #rnn_cell(module_name, hidden_dim)
        #])

        embedder_model = tf.keras.Sequential(name='embedder')
        for i in range(num_layers):
            embedder_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        #embedder_model.add(rnn_cell(module_name, hidden_dim))
        embedder_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        H = embedder_model(X, T)
        print(embedder_model.summary())
        e_vars = (embedder_model.trainable_variables)
        #the e_vars shape originally is sometimes (24, 48) but mine has (12, 48) two times, not sure if this makes a big diff??

        return H, e_vars


    def recovery (H, T):   
        """Recovery network from latent space to original space.
        
        Args:
        - H: latent representation
        - T: input time information
        
        Returns:
        - X_tilde: recovered data
        """     
        recovery_model = tf.keras.Sequential(name='recovery')
        for i in range(num_layers):
            recovery_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        recovery_model.add(tf.keras.layers.Dense(dim, activation='sigmoid'))

        X = recovery_model(H, T)
        #print(recovery_model.summary())
        r_vars = (recovery_model.trainable_variables)

        return X, r_vars
    
    def generator (Z, T):  
        """Generator function: Generate time-series data in latent space.
        
        Args:
        - Z: random variables
        - T: input time information
        
        Returns:
        - E: generated embedding
        """ 
        generator_model = tf.keras.Sequential(name='generator')
        for i in range(num_layers):
            generator_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        generator_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        E = generator_model(Z, T)
        #print(generator_model.summary())
        g_vars = (generator_model.trainable_variables)

        return E, g_vars

    def supervisor (H, T): 
        """Generate next sequence using the previous sequence.
        
        Args:
        - H: latent representation
        - T: input time information
        
        Returns:
        - S: generated sequence based on the latent representations generated by the generator
        """     
        supervisor_model = tf.keras.Sequential(name='supervisor')
        for i in range(num_layers-1):
            supervisor_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        supervisor_model.add(tf.keras.layers.Dense(hidden_dim, activation='sigmoid'))

        S = supervisor_model(H, T)
        #print(supervisor_model.summary())
        s_vars = (supervisor_model.trainable_variables)

        return S, s_vars

    def discriminator (H, T):   
        """Recovery network from latent space to original space.
        
        Args:
        - H: latent representation
        - T: input time information
        
        Returns:
        - X_tilde: recovered data
        """     
        discriminator_model = tf.keras.Sequential(name='discriminator')
        for i in range(num_layers):
            discriminator_model.add(rnn_cell(module_name, hidden_dim, return_sequences=True))
        discriminator_model.add(tf.keras.layers.Dense(dim, activation=None))

        X_tilde = discriminator_model(H, T)
        #print(discriminator_model.summary())
        d_vars = (discriminator_model.trainable_variables)

        #return X_tilde, discriminator_model (tried returning model and getting weights outside)
        #not sure if i should return weights now or get it later
        return X_tilde, d_vars

    H, e_vars = embedder(X, T)
    X_tilde, r_vars_1 = recovery(H, T)
    
    E_hat, g_vars = generator(Z, T)
    H_hat, s_vars_1 = supervisor(E_hat, T)
    H_hat_supervise, s_vars_2 = supervisor(H, T)

    X_hat, r_vars_2 = recovery(H_hat, T)

    # Discriminator
    Y_fake, d_vars_1 = discriminator(H_hat, T)    
    Y_real, d_vars_2 = discriminator(H, T)  
    Y_fake_e, d_vars_3 = discriminator(E_hat, T)

    # Variables      
    # not sure adding is right
    # original code seems to only have one set of variables for each type of model
    # i get one set each time the same model is called?? 
    e_vars_all = e_vars
    r_vars_all = r_vars_1 + r_vars_2 
    g_vars_all = g_vars
    s_vars_all = s_vars_1 + s_vars_2
    d_vars_all = d_vars_1 + d_vars_2 + d_vars_3

    # Discriminator loss
    # y fake, y real, y fake e all comes from discriminator
    # discriminator does not hv activation fn, so from_logits= True?
    #ori D_loss uses reduction SUM_BY_NONZERO_WEIGHTS, not sure if I should use AUTO or SUM_OVER_BATCH_SIZE or sth else
    #https://stackoverflow.com/questions/48418692/does-sigmoid-cross-entropy-produce-the-mean-loss-over-the-whole-batch
    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True) #loss for cls of latent real data seq
    #default arg for tf.keras.losses.BinaryCrossentropy reduction=losses_utils.ReductionV2.AUTO
    D_loss_real = bce(tf.ones_like(Y_real), Y_real)
    D_loss_fake = bce(tf.zeros_like(Y_fake), Y_fake) #loss for cls of latent synthethic data seq
    D_loss_fake_e = bce(tf.zeros_like(Y_fake_e), Y_fake_e) #loss for cls of latent synthetic data
    D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e

    # Generator loss
    # 1. Adversarial loss
    G_loss_U = bce(tf.ones_like(Y_fake), Y_fake) ##unsupervised (eq8)
    G_loss_U_e = bce(tf.ones_like(Y_fake_e), Y_fake_e)
    
    # 2. Supervised loss
    mse = tf.keras.losses.MeanSquaredError() #default: reduction=losses_utils.ReductionV2.AUTO
    #same as above, reduction method might be diff?
    G_loss_S = mse(H[:,1:,:], H_hat_supervise[:,:-1,:]) ##supervised loss (eq9) #loss for time series
    
    # 3. Two Momments
    G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X,[0])[1] + 1e-6)))
    G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat,[0])[0]) - (tf.nn.moments(X,[0])[0])))

    G_loss_V = G_loss_V1 + G_loss_V2 ##moment matching loss to improve diversity of generated sample

    # 4. Summation
    G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V

    # Embedder network loss
    E_loss_T0 = mse(X, X_tilde) ##reconstruction loss
    E_loss0 = 10*tf.sqrt(E_loss_T0)
    E_loss = E_loss0  + 0.1*G_loss_S

    def discriminator_loss(Y_real, Y_fake, Y_fake_e, gamma=gamma):
        bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
        D_loss_real = bce(tf.ones_like(Y_real), Y_real)
        D_loss_fake = bce(tf.zeros_like(Y_fake), Y_fake)
        D_loss_fake_e = bce(tf.zeros_like(Y_fake_e), Y_fake_e)
        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e
        return D_loss

    def generator_loss(Y_fake, H, X_hat, X, gamma=gamma):
        mse = tf.keras.losses.MeanSquaredError() 
        G_loss_S = mse(H[:,1:,:], H_hat_supervise[:,:-1,:]) ##supervised loss (eq9) #loss for time series
    
        G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X,[0])[1] + 1e-6)))
        G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat,[0])[0]) - (tf.nn.moments(X,[0])[0])))

        G_loss_V = G_loss_V1 + G_loss_V2 ##moment matching loss to improve diversity of generated sample
        
        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V
        return G_loss

    def generator_s_loss(Y_fake, H, X_hat, X, gamma=gamma):
        mse = tf.keras.losses.MeanSquaredError() 
        G_loss_S = mse(H[:,1:,:], H_hat_supervise[:,:-1,:])
        return G_loss_S

    def embedder_0_loss(X, X_tilde, H):
        #should we compute G_loss_S again?
        mse = tf.keras.losses.MeanSquaredError() 
        G_loss_S = mse(H[:,1:,:], H_hat_supervise[:,:-1,:])

        E_loss_T0 = mse(X, X_tilde) ##reconstruction loss
        E_loss0 = 10*tf.sqrt(E_loss_T0)
        #E_loss = E_loss0  + 0.1*G_loss_S
        return E_loss0

    # optimizer
    embedder_optimizer = tf.keras.optimizers.Adam()

    #for itt in range(iterations):
        # Set mini-batch
        #X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)

    @tf.function
    def train_step_embedder(X_mb, T_mb):

        with tf.GradientTape() as embedder_tape:
            H_mb, e_vars_mb = embedder(X_mb, T_mb)
            X_tilde_mb, r_vars_mb = recovery(H_mb, T_mb)

            embedder_0_loss = embedder_0_loss(X_mb, X_tilde_mb, H_mb)
            gradients_of_embedder = embedder_tape.gradient(embedder_0_loss, e_vars_mb)
            embedder_optmizer.apply_gradients(zip(gradients_of_embedder, e_vars_mb))
    
    def train():
        for itt in range(iterations):
            X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size) 
            print(X_mb, T_mb)
            train_step_embedder(X_mb, T_mb)
            print(itt)
    #train()

####TESTING####

from data_loading import real_data_loading, sine_data_generation

data_name = 'sine'
seq_len = 5

if data_name in ['stock', 'energy']:
  ori_data = real_data_loading(data_name, seq_len)
elif data_name == 'sine':
  # Set number of samples and its dimensions
  no, dim = 50, 2
  ori_data = sine_data_generation(no, seq_len, dim)
    
print(data_name + ' dataset is ready.')

## Newtork parameters
parameters = dict()

parameters['module'] = 'lstm' 
parameters['hidden_dim'] = 6
parameters['num_layer'] = 3
parameters['iterations'] = 10
parameters['batch_size'] = 4

timegan(ori_data, parameters)